@inproceedings{10.1145/3573051.3593375,
author = {Buckingham Shum, Simon},
title = {Trust, Sustainability and Learning@Scale},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593375},
doi = {10.1145/3573051.3593375},
abstract = {It is not overstating matters to say that humanity finds itself at an inflection point. The interlocking crises can feel overwhelming (ecological; political; financial; medical; technological; educational...), with recent leaps in AI closing the gap between human and machine cognition, raising many issues, including of course, educational questions. If a plausible diagnosis for our predicament as a species is "failure to learn", praxis questions assail us. What qualities should we cultivate most urgently, in which contexts? What literacies equip teachers and learners to engage critically with AI? How do we track progress meaningfully, at scale? And very pragmatically, when and why do people deem our tools trustworthy enough to trial, and if robust, embed sustainably into their teaching and learning practices?Taking a complex organisation as a microcosm of these challenges, I offer some reflections through the prism of nine years running a university learning analytics innovation centre. We invent and evaluate analytics and AI targeting student qualities that transcend the disciplines, such as critical and reflective writing, teamwork, dispositions, and sense of belonging. Without trust we cannot deploy at scale, motivating our use of methods from human-centered design and deliberative democracy to build common ground among diverse stakeholders.Perhaps the approaches and lessons learnt in this small but nonetheless complex system can scale fractally, offering insights for our wider challenges. Whatever the scale, it seems that trust, sustainability and learning reinforce each other, and must shape how we conceive, design and deploy our learning infrastructures.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {1–2},
numpages = {2},
keywords = {trust, sustainability, learning analytics, education, artificial intelligence},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596164,
author = {Poquet, Oleksandra},
title = {When Many Learners Interact: Towards Relational Processes at Scale},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596164},
doi = {10.1145/3573051.3596164},
abstract = {Over the past few decades, universities and adult learning environments have undergone significant changes. Class sizes have grown. Many students opt to work, limiting their on-campus engagement. Class attendance is hybrid. Study programs become more learner-customised modifying the notion of a cohort. Learner-machine interactions are not only possible but also easy to scale. These developments are changing the communal nature of learning and the opportunities for relational processes among learners. However, promoting relational processes is crucial for student well-being, academic achievement, and social capital. In this talk, I will argue for the need to design and support relational processes and present directions for future work in this area. Drawing on my research in different learning environments, I will focus on the evidence about the presence of relational processes at scale, as well as on how teacher and student behaviour affect them. First, I will discuss relational processes in large online groups in MOOCs on edX, Coursera, and Twitter. Then, I will describe the patterns of relational processes in university online discussions in a cross-institutional study and explain how teacher decisions may affect these patterns. Finally, I will discuss the role of student social learning strategies by drawing on a study of mobile communication among adult learners. Throughout these examples, I will reflect on the implications for pedagogy, institutional effort required to modify instructor behaviour, and interventions needed to support student social learning strategies that can spark relational processes.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {3},
numpages = {1},
keywords = {peer learning, learning analytics, learner relationships, learner networks, discussion forums},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593376,
author = {Prihar, Ethan and Haim, Aaron and Shen, Tracy and Sales, Adam and Lee, Dongwon and Wu, Xintao and Heffernan, Neil},
title = {Investigating the Impact of Skill-Related Videos on Online Learning},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593376},
doi = {10.1145/3573051.3593376},
abstract = {Many online learning platforms and MOOCs incorporate some amount of video-based content into their platform, but there are few randomized controlled experiments that evaluate the effectiveness of the different methods of video integration. Given the large amount of publicly available educational videos, an investigation into this content's impact on students could help lead to more effective and accessible video integration within learning platforms. In this work, a new feature was added into an existing online learning platform that allowed students to request skill-related videos while completing their online middle-school mathematics assignments. A total of 18,535 students participated in two large-scale randomized controlled experiments related to providing students with publicly available educational videos. The first experiment investigated the effect of providing students with the opportunity to request these videos, and the second experiment investigated the effect of using a multi-armed bandit algorithm to recommend relevant videos. Additionally, this work investigated which features of the videos were significantly predictive of students' performance and which features could be used to personalize students' learning. Ultimately, students were mostly disinterested in the skill-related videos, preferring instead to use the platforms existing problem-specific support, and there was no statistically significant findings in either experiment. Additionally, while no video features were significantly predictive of students' performance, two video features had significant qualitative interactions with students' prior knowledge, which showed that different content creators were more effective for different groups of students. These findings can be used to inform the design of future video-based features within online learning platforms and the creation of different educational videos specifically targeting higher or lower knowledge students. The data and code used in this work can be found at https://osf.io/cxkzf/.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {4–13},
numpages = {10},
keywords = {video tutoring, randomized controlled experiments, personalized learning, multi-armed bandit algorithms},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593392,
author = {Christie, S. Thomas and Johnson, Hayden and Cook, Carson and Gianopulos, Garron and Rafferty, Anna N.},
title = {LENS: Predictive Diagnostics for Flexible and Efficient Assessments},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593392},
doi = {10.1145/3573051.3593392},
abstract = {The utility of assessment systems lies in their capacity to transform observations of student behavior into meaningful inferences about learning, knowledge, and skills. Common practice is to use latent variable models and produce scores on scales. However the simplicity of these psychometric models may filter out potentially valuable information present in student behavior. In particular, scale scores are not optimized to support granular instructional decisions. Machine learning offers promising alternatives, but proposed deep learning architectures are not ideally suited for operational testing conditions involving sparse data and shifting category labels for test questions.We present criteria for a model architecture that can leverage the rich, sparse behavioral data made available by modern assessment systems. We argue that behavioral predictions with well-quantified uncertainties are a viable alternative to scale scores for many applications. To satisfy these goals, we propose the LENS model architecture that combines the flexibility of machine learning and the uncertainty quantification of latent variable models to produce high-quality predictive diagnostic claims. Inspired by variational autoencoders, LENS maps student behavior to a latent probability distribution. In addition, LENS performs explicit Bayesian integration of each behavioral observation, allowing the model to incorporate sparse data with informative prior beliefs about students.We then compare the predictive capability of LENS to both latent variable and machine learning approaches. LENS generates competitive or superior predictions of behavior, particularly on sparse data, while requiring fewer modeling assumptions than other options and allowing for easy incorporation of auxiliary behavioral data. We also show how to produce interpretable claims about student behavior at multiple levels of granularity, allowing the same model to serve multiple reporting needs. Finally, we discuss the flexibility afforded by the proposed model for constructing assessments customized to both educator needs and student skills.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {14–24},
numpages = {11},
keywords = {variational autoencoder, educational assessment, deep learning, Bayesian inference},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593396,
author = {Moore, Steven and Fang, Ellen and Nguyen, Huy A. and Stamper, John},
title = {Crowdsourcing the Evaluation of Multiple-Choice Questions Using Item-Writing Flaws and Bloom's Taxonomy},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593396},
doi = {10.1145/3573051.3593396},
abstract = {Multiple-choice questions, which are widely used in educational assessments, have the potential to negatively impact student learning and skew analytics when they contain item-writing flaws. Existing methods for evaluating multiple-choice questions in educational contexts tend to focus primarily on machine readability metrics, such as grammar, syntax, and formatting, without considering the intended use of the questions within course materials and their pedagogical implications. In this study, we present the results of crowdsourcing the evaluation of multiple-choice questions based on 15 common item-writing flaws. Through analysis of 80 crowdsourced evaluations on questions from the domains of calculus and chemistry, we found that crowdworkers were able to accurately evaluate the questions, matching 75\% of the expert evaluations across multiple questions. They were able to correctly distinguish between two levels of Bloom's Taxonomy for the calculus questions, but were less accurate for chemistry questions. We discuss how to scale this question evaluation process and the implications it has across other domains. This work demonstrates how crowdworkers can be leveraged in the quality evaluation of educational questions, regardless of prior experience or domain knowledge.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {25–34},
numpages = {10},
keywords = {question quality, question generation, question evaluation, learnersourcing, crowdsourcing},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593377,
author = {Gillani, Nabeel and Beeferman, Doug and Overney, Cassandra and Vega-Pourheydarian, Christine and Roy, Deb},
title = {All A-board: Sharing Educational Data Science Research with School Districts},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593377},
doi = {10.1145/3573051.3593377},
abstract = {Educational data scientists often conduct research with the hopes of translating findings into lasting change through policy, civil society, or other channels. However, the bridge from research to practice can be fraught with sociopolitical frictions that impede, or altogether block, such translations-especially when they are contentious or otherwise difficult to achieve. Focusing on one entrenched educational equity issue in US public schools-racial and ethnic segregation-we conduct randomized email outreach experiments and surveys to explore how local school districts respond to algorithmically-generated school catchment areas ("attendance boundaries") designed to foster more diverse and integrated schools. Cold email outreach to approximately 4,320 elected school board members across over 800 school districts informing them of potential boundary changes reveals a large average open rate of nearly 40\%, but a relatively small click-through rate of 2.5\% to an interactive dashboard depicting such changes. Board members, however, appear responsive to different messaging techniques---particularly those that dovetail issues of racial and ethnic diversity with other top-of-mind issues (like school capacity planning). On the other hand, media coverage of the research drives more dashboard engagement, especially in more segregated districts. A small but rich set of survey responses from school board and community members across several districts identify data and operational bottlenecks to implementing boundary changes to foster more diverse schools, but also share affirmative comments on the potential viability of such changes. Together, our findings may support educational data scientists in more effectively disseminating research that aims to bridge educational inequalities through systems-level change.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {35–47},
numpages = {13},
keywords = {segregation, randomized experiments, inequality, education},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593381,
author = {Deho, Oscar Blessed and Joksimovic, Srecko and Liu, Lin and Li, Jiuyong and Zhan, Chen and Liu, Jixue},
title = {Assessing the Fairness of Course Success Prediction Models in the Face of (Un)equal Demographic Group Distribution},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593381},
doi = {10.1145/3573051.3593381},
abstract = {In recent years, predictive models have been increasingly used by education practitioners and stakeholders to leverage actionable insights to support student success. Usually, model selection (i.e., the decision of which predictive model to use) is largely based on the predictive performance of the models. Nevertheless, it has become important to consider fairness as an integral part of the criteria for model selection. Might a model be unfair towards certain demographic groups? Might it systematically perform poorly for certain demographic groups? Indeed, prior studies affirm this. Which model out of the lot should we choose then? Additionally, prior studies suggest demographic group imbalance in the training dataset to be a source of such unfairness. If so, would the fairness of the predictive models improve if the demographic group distribution in the training dataset becomes balanced? This study seeks to answer these questions. Firstly, we analyze the fairness of 4 of the commonly used state-of-the-art models to predict course success for 3 IT courses in a large public Australian university. Specifically, we investigate if the models serve different demographic groups equally. Secondly, to address the identified unfairnes---supposedly caused by the demographic group imbalance---we train the models on 3 types ofbalanced data and investigate again if the unfairness was mitigated. We found that none of the predictive models wasconsistently fair in all 3 courses. This suggests that model selection decision should be carefully made by both the researchers and stakeholders as the per the requirement of the domain of application. Furthermore, we found that balancing demographic groups (and class labels) is not enough---albeit can be an initial step---to ensure fairness of predictive models in education. An implication of this is that sometimes, the source of unfairness may not be immediately apparent. Therefore, "blindly" attributing the unfairness to demographic group imbalance may cause the unfairness to persist even when the data becomes balanced. We hope that our findings can guide practitioners and relevant stakeholders in making well-informed decisions.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {48–58},
numpages = {11},
keywords = {protected group imbalance, fairness in learning analytics, course success prediction},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593379,
author = {Demszky, Dorottya and Liu, Jing},
title = {M-Powering Teachers: Natural Language Processing Powered Feedback Improves 1:1 Instruction and Student Outcomes},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593379},
doi = {10.1145/3573051.3593379},
abstract = {Although learners are being connected 1:1 with instructors at an increasing scale, most of these instructors do not receive effective, consistent feedback to help them improve. We deployed M-Powering Teachers, an automated tool based on natural language processing to give instructors feedback on dialogic instructional practices ---including their uptake of student contributions, talk time and questioning practices --- in a 1:1 online learning context. We conducted a randomized controlled trial on Polygence, a research mentorship platform for high schoolers (n=414 mentors) to evaluate the effectiveness of the feedback tool. We find that the intervention improved mentors' uptake of student contributions by 10\%, reduced their talk time by 5\% and improved student's experience with the program as well as their relative optimism about their academic future. These results corroborate existing evidence that scalable and low-cost automated feedback can improve instruction and learning in online educational contexts.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {59–69},
numpages = {11},
keywords = {randomized controlled trial, natural language processing, automated teacher feedback},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593390,
author = {Gurung, Ashish and Baral, Sami and Lee, Morgan P. and Sales, Adam C. and Haim, Aaron and Vanacore, Kirk P. and McReynolds, Andrew A. and Kreisberg, Hilary and Heffernan, Cristina and Heffernan, Neil T.},
title = {How Common are Common Wrong Answers? Crowdsourcing Remediation at Scale},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593390},
doi = {10.1145/3573051.3593390},
abstract = {Solving mathematical problems is cognitively complex, involving strategy formulation, solution development, and the application of learned concepts. However, gaps in students' knowledge or weakly grasped concepts can lead to errors. Teachers play a crucial role in predicting and addressing these difficulties, which directly influence learning outcomes. However, preemptively identifying misconceptions leading to errors can be challenging. This study leverages historical data to assist teachers in recognizing common errors and addressing gaps in knowledge through feedback. We present a longitudinal analysis of incorrect answers from the 2015-2020 academic years on two curricula, Illustrative Math and EngageNY, for grades 6, 7, and 8. We find consistent errors across 5 years despite varying student and teacher populations. Based on these Common Wrong Answers (CWAs), we designed a crowdsourcing platform for teachers to provide Common Wrong Answer Feedback (CWAF). This paper reports on an in vivo randomized study testing the effectiveness of CWAFs in two scenarios: next-problem-correctness within-skill and next-problem-correctness within-assignment, regardless of the skill. We find that receiving CWAF leads to a significant increase in correctness for consecutive problems within-skill. However, the effect was not significant for all consecutive problems within-assignment, irrespective of the associated skill. This paper investigates the potential of scalable approaches in identifying Common Wrong Answers (CWAs) and how the use of crowdsourced CWAFs can enhance student learning through remediation.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {70–80},
numpages = {11},
keywords = {feedback intervention theory, engineering feedback at scale, common wrong answers, buggy message},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593391,
author = {Kim, Yunsung and Piech, Chris},
title = {High-Resolution Course Feedback: Timely Feedback Mechanism for Instructors},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593391},
doi = {10.1145/3573051.3593391},
abstract = {We study the problem of minimizing the delay between when an issue comes up in a course and when instructors get feedback about it. The widespread practice of obtaining midterm and end-of-term feedback from students is suboptimal in this regard, especially for large courses: it over-samples at a specific point in the course and can be biased by factors irrelevant to the teaching process. As a solution, we release High Resolution Course Feedback (HRCF), an open-source student feedback mechanism that builds on a surprisingly simple idea: survey each student on random weeks exactly twice per term. Despite the simplicity of its core idea, when deployed to 31 courses totaling a cumulative 6,835 students, HRCF was able to detect meaningful mood changes in courses and significantly improve timely feedback without asking for extra work from students compared to the common practice. An interview with the instructors revealed that HRCF provided constructive and useful feedback about their courses early enough to be acted upon, which would have otherwise been unobtainable through other survey methods. We also explore the possibility of using Large Language Models to flexibly and intuitively organize large volumes of student feedback at scale and discuss how HRCF can be further improved.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {81–91},
numpages = {11},
keywords = {timely feedback, student feedback on teaching, student evaluations of teaching, course survey, course improvement},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593385,
author = {Hillman, Thomas and Seredko, Alena and Osborne, Tanya Karin and Nivala, Markus},
title = {From Answering for Points to Commenting for Others},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593385},
doi = {10.1145/3573051.3593385},
abstract = {In this paper, we examine posting behavior on a large-scale online knowledge sharing platform, Stack Overflow, and investigate how that behavior changes as users gain experience and status. We use a trace ethnographic approach, analyzing both the sayings and doings of users and the numerical and categorical trace data produced by their activities on the platform. Through analysis of this data, as well as interviews with users at different levels of experience, we identify a pattern of behavior where users shift their focus over time from directly answering questions themselves to supporting others and maintaining the quality of knowledge on the platform thus playing a particular role as the knowledge community scales},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {92–98},
numpages = {7},
keywords = {trace ethnography, stack overflow, posting behavior, knowledge sharing},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593386,
author = {Doroudi, Shayan and Ahmad, Yusuf},
title = {The Relevance of Ivan Illich's Learning Webs 50 Years On},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593386},
doi = {10.1145/3573051.3593386},
abstract = {In 1971, social critic Ivan Illich published Deschooling Society, a controversial work that critiqued mainstream education systems and proposed a radical alternative. While his work remains controversial, re-examining his ideas might advance efforts to design for learning at scale. First, we examine three design principles that emerge from Illich's writing on learning webs: (1) a holistic perspective that incorporates multidisciplinary thinking (in Illich's case, blending philosophy, politics, sociology, economics, theology, and cybernetics), (2) learning webs as a framework for thinking about learning beyond the limitations of school, and (3) broadening our view of what should be scaled (e.g., scaling opportunities rather than content). Second, we discuss three tensions in Illich's work that relate to scaling learning: (1) decentralization vs. centralization, (2) place-based vs. online learning at scale, and (3) serving the advantaged vs. the disadvantaged. In discussing these ideas and tensions, we discuss contemporary technologies and models, which may be seen as similar to learning webs. Finally, we suggest that Illich's work offers an opportunity to further connect work that sits across two related research communities: Learning @ Scale and connected learning.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {99–109},
numpages = {11},
keywords = {peer learning, learning exchanges, connected learning, Ivan Illich},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593383,
author = {Battestilli, Lina and Boh\'{o}rquez, Elaine B. and Khan, Sarah and Meral, Cigdem},
title = {Exploring Students' Perceptions and Engagement in Hybrid Flexible Courses},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593383},
doi = {10.1145/3573051.3593383},
abstract = {The Hybrid Flexible (HyFlex) instruction format provides learners with the flexibility to choose from in-person, online synchronous, or asynchronous learning. However, students' learning experiences with HyFlex has not been studied at scale. The primary goal of this study was to investigate how students' perceptions about the availability of learning resources relates to their course engagement and performance in a HyFlex learning environment. In Spring 2022, we administered an end-of-semester survey to one graduate and five undergraduate courses, each of which utilized the HyFlex instructional model. Courses were selected from three different colleges at a large public university in the United States. We investigated students' perceptions about the effectiveness, importance, and ease of use of all three learning modalities that were offered (in-person, online synchronous, and asynchronous) and the learning support options (instructor access outside of class, learning help resources, and flexibility to choose learning modality without restriction). With a sample size of 537, we found that 30\% of surveyed students found in-person and online synchronous learning important for their learning whereas 60\% found asynchronous learning and the flexibility to choose their learning modality important for their learning. When asked about their actual use of different modalities, students reported using asynchronous learning the most, followed by online synchronous learning. In-person learning was reportedly the least utilized. We found that non-real-time learning modalities contributed positively to overall student engagement. Students preferred to use asynchronous resources and have the flexibility to choose among learning modalities. Yet, results indicate that students who incorporated some real-time learning not only had higher performance-related engagement (e.g., confidence in their ability to succeed in the course) than those who relied primarily on non-real-time learning, but they also earned higher grades for the course. This suggests that utilizing some in-person or online synchronous modalities in conjunction with the student-preferred asynchronous options leads to improved course outcomes for both student engagement and course performance.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {110–119},
numpages = {10},
keywords = {student performance, student engagement, hyflex, hybrid flexible},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593384,
author = {McDonald, Emma and Arevalo, Gisele and Ahmed, Sadaf and Akhmetov, Ildar and Demmans Epp, Carrie},
title = {Managing TAs at Scale: Investigating the Experiences of Teaching Assistants in Introductory Computer Science},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593384},
doi = {10.1145/3573051.3593384},
abstract = {Teaching assistants (TAs) are essential members of post-secondary instructional teams, who often have considerable student-facing time. The recently increasing demand for introductory computer-science (CS) courses has resulted in a corresponding increased demand for TAs. The existing TA literature has predominantly focused on investigating the role of training in the TA experience, particularly with respect to performance. This provides little insight into how TAs experience their management. Consequently, we investigate the role of management in the experiences of introductory-level CS TAs. We provide the structure for a formal, tiered management scheme employed in a high-enrolment introductory CS course. This scheme attempts to address the challenges associated with managing TAs at scale. As a case study, we compare the experience of TAs under this new management scheme with that of TAs under an informal, unstructured management style used in smaller introductory CS courses. Specifically, we used questionnaires to look at TAs' self-efficacy and understand their experiences. While we did not find a significant difference in TA self-efficacy between the two management styles, thematic analysis of the open-response data revealed a greater number of challenges reported by the TAs under the tiered management system. TAs characterized this tiered system as "organized" and frequently reported feeling overworked. Across both groups, TAs identified improved communication and additional training as factors that could improve their experience. Our findings suggest self-efficacy was not a sufficient measure to quantify TA experience. We propose framing TA experience based on their motivation and general well-being.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {120–131},
numpages = {12},
keywords = {self-efficacy, TA management, CS education},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593388,
author = {Zhou, Xiaofei and Kok, Christopher and Quintana, Rebecca M. and Delahay, Anita and Wang, Xu},
title = {How Learning Experience Designers Make Design Decisions: The Role of Data, the Reliance on Subject Matter Expertise, and the Opportunities for Data-Driven Support},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593388},
doi = {10.1145/3573051.3593388},
abstract = {Learning Experience Designers (LXDs) play an increasingly consequential role in the creation of courses and training materials that meet the needs of diverse learner populations and the growing class scope. Emerging design requests for scalable and effective courseware introduce new challenges in Learning Experience (LX) design practice while providing an opportunity for researchers to understand LX workflows and design new tools to improve them. This paper presents an interview study with 21 LXDs from 18 different organizations with the goal of understanding LXDs' collaborative relationships with subject matter experts (SMEs), data needs, and contextual challenges. We further perform a survey study to validate the challenges and probe into LXDs' attitudes toward a suite of data-driven solutions. We find that LXDs demonstrate a strong desire to collect data to inform their design - including target learners' prior knowledge and relevant design precedents. LXDs want support in better collaborating with SMEs, acquiring and processing diverse learner data, identifying relevant research studies to communicate their design decisions, understanding domain-specific material, and creating quality materials (especially questions). We discuss LXDs' concerns regarding automated solutions such as the lack of contextual understanding, over-reliance on automation, and data privacy before elaborating on the implications for future work.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {132–143},
numpages = {12},
keywords = {survey, learning experience design, interview, instructional design, data access},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593389,
author = {Joyner, David A. and Rusch, Ana and Duncan, Alex and Wojcik, Jolanta and Popescu, Diana},
title = {Teaching at Scale and Back Again: The Impact of Instructors' Participation in At-Scale Education Initiatives on Traditional Instruction},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593389},
doi = {10.1145/3573051.3593389},
abstract = {Proponents of at-scale education initiatives often tout the benefits of these programs to the students who enroll in them, but anecdotally, faculty who participate in creating at-scale courses have often commented on how their participation positively impacted their subsequent in-person teaching. In this study, we investigate this potential phenomenon more thoroughly to assess how generalizable this perspective is among participants in these initiatives. We conduct three studies: we interview 78 faculty teaching in at-scale degree programs, examine offering histories for 70 courses offered in one such program, and survey 153 faculty who have developed massive open online courses. Based on these three studies, we propose a list of ways in which in-person instruction benefits from the existence of at-scale teaching initiatives. We further discuss remaining perceived drawbacks of at-scale education as derived from these interviews and surveys.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {144–155},
numpages = {12},
keywords = {in-person instruction, affordable degrees at scale, MOOCs},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593394,
author = {Leite, Walter L. and Hatch, Amber D. and Kuang, Huan and Cavanaugh, Catherine and Xing, Wanli},
title = {How Teachers Influence Student Adoption and Effectiveness of a Recommendation System for Algebra},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593394},
doi = {10.1145/3573051.3593394},
abstract = {Advanced learning technologies (ALT) have become increasingly available to teachers for classroom use. Research has suggested that many factors can influence teacher adoption and fidelity of use of ALT in the classroom, including teacher beliefs, knowledge and experience, technological factors, and instructional factors. However, there has been scarce research linking teacher factors to student adoption of ALTs. This study examined the relationships between teacher characteristics and practices and student adoption and learning gains with a video recommendation system embedded within a virtual learning environment (VLE) for Algebra. Secondary data was obtained from an experimental study conducted over one academic semester in middle and high schools in a southeastern state of the United States. The sample included 52 teachers and 2936 students. The data included teacher responses to three surveys, and student demographic and achievement variables. A random forest was used to predict the rate that the students followed video recommendations in the VLE. The results show that the recommendation followed rate is related to the teachers' fidelity of use, frequency of student monitoring, and experience with the VLE. Most of the survey items specifically evaluating teachers' beliefs about the recommender were important predictors of students' following video recommendations. Teacher monitoring through a dashboard was the most important predictor. The analysis of treatment effect heterogeneity of the video recommendation system was performed using the generic machine learning inference (GenericML) method paired with random forests. Results show that teachers of students who benefitted most reported spending more time using the videos of the VLE and following student progress through the dashboard, but less time on the VLE than teachers of students who benefit the least. Teachers of students who benefitted the least had larger classrooms, struggled more with the challenges due to the Covid-19 pandemic, and spent less time with classroom planning. The results support the recommendation that teacher professional development for ALT should engage groups of educators in increasing their experience with the application so that they build comfort and confidence in its use in ways in which students are most likely to benefit.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {156–163},
numpages = {8},
keywords = {video recommendation system, random forest, heterogeneity of treatment effect, generic machine learning inference (GenericML)},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593395,
author = {Vanacore, Kirk and Sales, Adam and Liu, Allison and Ottmar, Erin},
title = {Benefit of Gamification for Persistent Learners: Propensity to Replay Problems Moderates Algebra-Game Effectiveness},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593395},
doi = {10.1145/3573051.3593395},
abstract = {Computer-assisted learning platforms (CALPS) increasingly include gamified elements to improve student outcomes by enhancing their engagement with content. Although evidence exists that gamified programs increase engagement and learning outcomes, there is little causal research on what programmatic mechanisms drive the effect between engagement and learning. In the following paper, we explore this relationship through a method of causal moderation known as fully latent principal stratification. Using data from a large-scale randomized control trial assessing gamified and traditional CALP systems' effects on algebraic knowledge, we estimate the impact of using the gamified CALP on students who engage with one of its key gamification elements---replaying a problem after a suboptimal attempt. The gamified CALP asks students to manipulate algebraic expressions from start to goal states and provides feedback based on the efficiency of these manipulations, allowing students to replay the problems when their efficiency can be improved. We find that the effect of gamification is greater for students with a higher propensity to replay problems. This finding suggests that gamification elements that provide students with opportunities to retry problems are driving the game's efficacy and provide evidence for a scalable mechanism of gamification that can improve students' learning.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {164–173},
numpages = {10},
keywords = {productive persistence, gamification, computer-assisted learning platforms, causal inference},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596166,
author = {Haim, Aaron and Baxter, Chris and Gyurcsan, Robert and Shaw, Stacy T. and Heffernan, Neil T.},
title = {How to Open Science: Analyzing the Open Science Statement Compliance of the Learning @ Scale Conference},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596166},
doi = {10.1145/3573051.3596166},
abstract = {There have been numerous efforts documenting the effects of open science in existing papers; however, these efforts typically only consider the author's analyses and supplemental materials from the papers. While understanding the current rate of open science adoption is important, it is also vital that we explore the factors that may encourage such adoption. One such factor may be publishing organizations setting open science requirements for submitted articles: encouraging researchers to adopt more rigorous reporting and research practices. For example, within the education technology discipline, theACM Conference on Learning @ Scale (L@S) has been promoting open science practices since 2018 through a Call For Papers statement. The purpose of this study was to replicate previous papers within the proceedings of L@S and compare the degree of open science adoption and robust reproducibility practices to other conferences in education technology without a statement on open science. Specifically, we examined 93 papers and documented the open science practices used. We then attempted to reproduce the results with invitation from authors to bolster the chance of success. Finally, we compared the overall adoption rates to those from other conferences in education technology. Although the overall responses to the survey were low, our cursory review suggests that researchers at L@S might be more familiar with open science practices compared to the researchers who published in theInternational Conference on Artificial Intelligence in Education (AIED) and theInternational Conference on Educational Data Mining (EDM): 13 of 28 AIED and EDM responses were unfamiliar with preregistrations and 7 unfamiliar with preprints, while only 2 of 7 L@S responses were unfamiliar with preregistrations and 0 with preprints. The overall adoption of open science practices at L@S was much lower with only 1\% of papers providing open data, 5\% providing open materials, and no papers had a preregistration.All openly accessible work can be found in an Open Science Framework project.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {174–182},
numpages = {9},
keywords = {statement compliance, reproducibility, peer review, open science},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593380,
author = {Andres-Bray, Juan-Miguel and Hutt, Stephen and Baker, Ryan S.},
title = {Exploring Cross-Country Prediction Model Generalizability in MOOCs},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593380},
doi = {10.1145/3573051.3593380},
abstract = {Massive Open Online Courses (MOOCs) have increased the accessibility of quality educational content to a broader audience across a global network. They provide access for students to material that would be difficult to obtain locally, and an abundance of data for educational researchers. Despite the international reach of MOOCs, however, the majority of MOOC research does not account for demographic differences relating to the learners' country of origin or cultural background, which have been shown to have implications on the robustness of predictive models and interventions. This paper presents an exploration into the role of nation-level metrics of culture, happiness, wealth, and size on the generalizability of completion prediction models across countries. The findings indicate that various dimensions of culture are predictive of cross-country model generalizability. Specifically, learners from indulgent, collectivist, uncertainty-accepting, or short-term oriented, countries produce more generalizable predictive models of learner completion.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {183–194},
numpages = {12},
keywords = {predictive modeling, generalizability, cross-culture, cross-country, completion, MORF, MOOCs},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593382,
author = {Lee, Hansol and Kizilcec, Ren\'{e} F. and Joachims, Thorsten},
title = {Evaluating a Learned Admission-Prediction Model as a Replacement for Standardized Tests in College Admissions},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593382},
doi = {10.1145/3573051.3593382},
abstract = {A growing number of college applications has presented an annual challenge for college admissions in the United States. Admission offices have historically relied on standardized test scores to organize large applicant pools into viable subsets for review. However, this approach may be subject to bias in test scores and selection bias in test-taking with recent trends toward test-optional admission. We explore a machine learning-based approach to replace the role of standardized tests in subset generation while taking into account a wide range of factors extracted from student applications to support a more holistic review. We evaluate the approach on data from an undergraduate admission office at a selective US institution (13,248 applications). We find that a prediction model trained on past admission data outperforms an SAT-based heuristic and matches the demographic composition of the last admitted class. We discuss the risks and opportunities for how such a learned model could be leveraged to support human decision-making in college admissions.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {195–203},
numpages = {9},
keywords = {standardized testing, predictive modeling, machine learning, higher education, college admissions},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593387,
author = {Park, Jungkook and Oh, Alice},
title = {EliRank: A Code Editing History Based Ranking Model for Early Detection of Students in Need},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593387},
doi = {10.1145/3573051.3593387},
abstract = {Research on programming education shows that novice programming students benefit significantly from one-to-one tutoring. While many systems propose to replicate the effectiveness of one-to-one tutoring in large-scale classes, it remains a challenge to develop systems with an approach to finding students who need the tutors' help the most. In this paper, we explore the idea of predicting the priority of students in need with a data-driven approach. Among various metrics to calculate the priority of students in need, we adopt time-on-task metric. Previous studies have found that excessively long time-on-task can be used as an indication of students' struggling. Aligned with this, we reduce the problem of finding students with the highest priority to the problem of finding students with the longest time-on-task. To solve the reduced problem, we present EliRank, a ranking model that finds students with the longest estimated time-on-task, using the students' first few minutes of fine-grained code editing history. EliRank recommends students in the descending order of estimated time-on-task, enabling tutors to efficiently monitor and find the students in need at scale in real time. To evaluate the performance of EliRank, we build and publish a new real-world dataset consisting of 15 programming exercises solved by 4000+ students in an introduction to programming class at a university. Unlike the currently available open code editing history datasets, our dataset contains code editing operations at a character-level granularity to minimize the loss of contextual information from students. We also introduce diff-augmented abstract syntax tree (DAST), a novel structured code representation that minimizes the loss of fine-grained code change information during code parsing. The evaluation of EliRank on our dataset shows that EliRank effectively finds students with the longest estimated time-on-task, for early detection of students in need. Also, we illustrate in depth (i) the effectiveness of DAST, (ii) the potential to control the tradeoff between early detection and the prediction accuracy of the model, and (iii) the transferability to unseen programming exercise via zero-shot transfer learning.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {204–214},
numpages = {11},
keywords = {recommendation system, programming education, machine learning, code editing history},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593378,
author = {Xu, Lingrui and Pardos, Zachary A. and Pai, Anirudh},
title = {Convincing the Expert: Reducing Algorithm Aversion in Administrative Higher Education Decision-making},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593378},
doi = {10.1145/3573051.3593378},
abstract = {Algorithm aversion can be described as the tendency of human decision-makers to discount algorithmic recommendations more heavily than similar recommendations made by humans. It has been a phenomenon observed to be most acutely exhibited by domain experts. In our work, we focus on expert administrators in higher education making course credit equivalency decisions that affect the academic planning and potential degree progress of millions of prospective transfer students. Using human-centered design, we construct an AI-based platform for recommending matches to courses on a student's transcript to courses offered at another institution. We conduct a 2 x 2, between-subject experiment to investigate potential aversion mitigation techniques by manipulating the presence of outliers and allowing users to provide feedback to the algorithm. Our findings indicate that intentional, human-centered design and careful presentation of algorithm-based recommendations can help improve Human-AI interaction and productivity with implications for various domains of expertise.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {215–225},
numpages = {11},
keywords = {human-AI collaboration, higher education, credit mobility, articulation, algorithm aversion},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593393,
author = {Markel, Julia M. and Opferman, Steven G. and Landay, James A. and Piech, Chris},
title = {GPTeach: Interactive TA Training with GPT-based Students},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593393},
doi = {10.1145/3573051.3593393},
abstract = {Interactive and realistic teacher training is hard to scale. This is a key issue for learning at scale, as inadequate preparation can negatively impact both students and teachers. What if we could make the teacher training experience more engaging and, as a downstream effect, reduce the potential for harm that teachers-in-training could inflict on students? We present GPTeach, an interactive chat-based teacher training tool that allows novice teachers to practice with simulated students. We performed two studies to evaluate GPTeach: one think-aloud study and one A/B test between our tool and a baseline. Participants took the role of a teaching assistant conducting office hours with two GPT-simulated students. We found that our tool provides the opportunity for teachers to get valuable teaching practice without the pressures of affecting real students, allowing them to iterate their responses both during and across sessions. Additionally, participants enjoyed flexibility in tailoring their responses according to the varied personas, needs, and learning goals. In this paper, we provide quantitative results and qualitative observations to inform future work in this area. We conclude with a discussion of actionable design ideas for such systems, as well as other ways to use this tool for evaluating teachers and students. GPTeach has recently been deployed into the teacher training component of an online course with over 800 novice teachers.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {226–236},
numpages = {11},
keywords = {scalable teacher training, GPT-simulated students},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596165,
author = {Ochoa, Xavier and Echeverria, Vanessa and Carrillo, Gladys and Heredia, Vanessa and Chiluiza, Katherine},
title = {Supporting Online Collaborative Work at Scale: A Mixed-Methods Study of a Learning Analytics Tool},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596165},
doi = {10.1145/3573051.3596165},
abstract = {Collaborative Learning Analytics (CLA) tools have recently emerged as a potential solution to address the onerous process of monitoring and providing timely feedback on collaboration skills in higher education students. However, prior studies on the efficacy of such tools have mainly been carried out in small, controlled settings. This study aims to measure the impact of a specific CLA tool that can be easily deployed on a larger scale with minimal instructor effort in real-world online group work activities. Additionally, this research examines the potential influence that the characteristics of the collaborative activity may have on the tool's effectiveness. The CLA tool under investigation displays speaking participation time and peer evaluation scores from students engaged in online collaborative activities as part of their regular courses. The tool was evaluated with five instructors and 156 students over the course of one semester. The effects of the tool on students' speaking participation and peer evaluation scores were quantitatively measured and tested. A qualitative analysis of reflections from both students and instructors provided supplementary information on the quantitative results. The main finding of this study indicates that the tool has an overall small positive impact. The effectiveness of the CLA tool is primarily modulated by the synchronous or asynchronous presence of the instructor, as students tend to interact more naturally and feel less scrutinized in the absence of instructor evaluation. Based on the discussion of the findings, this research suggests design insights to enhance future CLA tools at scale for the purpose of supporting the development of online collaboration skills.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {237–247},
numpages = {11},
keywords = {participation time, online collaboration, collaboration skills},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593398,
author = {Haim, Aaron and Shaw, Stacy T. and Heffernan, Neil T.},
title = {How to Open Science: Promoting Principles and Reproducibility Practices within the Learning @ Scale Community},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593398},
doi = {10.1145/3573051.3593398},
abstract = {Across the past decade, open science has increased in momentum, making research more openly available and reproducible. In addition, learning at scale systems have been developed to collect and apply models, features and reports to better support students and teachers towards their goals. In this tutorial, we will provide an overview of open science practices and their benefits and mitigation within research. In the second part of this tutorial, we will use the Open Science Framework to make, collaborate, and share projects - demonstrating how to make materials, code, and data open. The final part of this tutorial will go over some mitigation strategies when releasing datasets and materials so other researchers may easily reproduce them. Participants in this tutorial learn what the practices of open science are, how to use them in their own research, and how to use the Open Science Framework.The website1 and associated resources can be found on an Open Science Framework project2.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {248–250},
numpages = {3},
keywords = {reproducibility, preregistration, open science},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593399,
author = {Anastasopoulos, Ioannis and Sheel, Shreya and Pardos, Zachary and Bhandari, Shreya},
title = {Introducing an Open-source Adaptive Tutoring System to Accelerate Learning Sciences Experimentation},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593399},
doi = {10.1145/3573051.3593399},
abstract = {Learning @ Scale has embraced movements that spread access to education through open and free platforms of learning. In this tutorial, we introduce OATutor (recently published at CHI'23), the field's first free and open-source adaptive tutoring system based on ITS principles and designed for rapid experimentation. The MIT-licensed platform can be deployed to git-pages in only a few clicks and supports BKT mastery-based adaptive problem selection. We demonstrate how the system can be used to rapidly run A/B experiments, analyze the data, and publish the entire tutor, content, and analysis scripts to facilitate unprecedented ease of replication and transparency, as demonstrated in a recent study comparing ChatGPT generated hints to human-tutor hints. Our four-part tutorial will include how to add lessons to the system and link to them from assignments in a MOOC platform or LMS via LTI. The structured JSON format of the four CC BY courses worth of content released with OATutor opens up avenues for researchers to apply new and existing educational data mining and NLP techniques (e.g., KC tagging) and rapidly evaluate the impact of subsequent changes on learners.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {251–253},
numpages = {3},
keywords = {intelligent tutoring systems, creative commons, authoring, HCI, A/B testing},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3593397,
author = {Ritter, Steve and Heffernan, Neil and Williams, Joseph Jay and Lomas, Derek and Bicknell, Klinton and Roschelle, Jeremy and Motz, Ben and McNamara, Danielle and Baraniuk, Richard and Basu Mallick, Debshila and Kizilcec, Rene and Baker, Ryan and Fancsali, Stephen and Murphy, April},
title = {Fourth Annual Workshop on A/B Testing and Platform-Enabled Learning Research},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593397},
doi = {10.1145/3573051.3593397},
abstract = {Learning engineering adds tools and processes to learning platforms to support improvement research. One kind of tool is A/B testing-common in large software companies and also represented academically at conferences like the Annual Conference on Digital Experimentation (CODE). A number of A/B testing systems focused on educational apps have arisen recently, including UpGrade and E-TRIALS. A/B testing can help improve educational platforms, yet challenging issues in education go beyond the generic paradigm. In response, a number of of digital learning platforms is opening their systems to learning-improvement research by instructors and/or third-party researchers, with specific supports necessary for education-specific research designs. This workshop will explore how A/B testing in educational contexts is different, how learning platforms are opening up new possibilities, and how these empirical approaches can be used to drive powerful gains in student learning. It will also discuss forthcoming opportunities for funding to conduct platform-enabled learning research.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {254–256},
numpages = {3},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596167,
author = {Rusch, Ana Mary and Duncan, Alex S. and Joyner, David A.},
title = {Student Life at Scale: Humanizing the Student Experience at Scale through Belonging, Engagement, and Community},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596167},
doi = {10.1145/3573051.3596167},
abstract = {Using a Computer Science affordable degree at scale as a case study, the context of this paper explores the ways in which student life initiatives can humanize the at scale and online distance learning student experience through a holistic and student care-centric approach. By analyzing the top ten student life initiatives featured in an online at scale computer science program, we highlight ways to ensure belonging, facilitate engagement, and create community at scale while tackling inherent problems of online distance learning such as isolation and social disconnectedness.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {257–261},
numpages = {5},
keywords = {student life at scale, online education, diversity and inclusion, computer science graduate program, community and belonging, at scale learning},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596168,
author = {Chrysilla, Grace Naomi and Joyner, David},
title = {Utilizing Neural Network to Predict Students Aptitudes for Teaching Assistant Roles},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596168},
doi = {10.1145/3573051.3596168},
abstract = {The emerging field of affordable degrees at scale relies in large part on a linear relationship between the number of students who enroll and the number of teaching assistants who are employed. As programs grow, however, identifying good candidates can become untenable: research has found that the fraction of students who apply for TA roles far exceeds the number of actual available roles, forcing instructors to comb through hundreds of applications for a tiny number of positions. Most of these applicants are themselves former students in the class as well, meaning that there is abundant data from their assignments, grades, peer review behaviors, and forum participation to evaluate candidates, but navigating and using this data requires significant time.In this work, we utilize machine learning to predict students' aptitudes for teaching assistant roles. We train a neural network model on the available data and aggregate meaningful features to determine if a student possesses the qualities associated with being a good TA. This paper covers implementation details of the training dataset, neural network model training, model result and analysis. We also present a verification method and suggest future improvements.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {262–266},
numpages = {5},
keywords = {teaching assistant, online learning, neural network, machine learning, higher education, education, distance learning, artificial intelligence},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596169,
author = {Pham, Trang Thi},
title = {Digital Access, Usage, and Educational Outcomes: Evidence from Vietnam},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596169},
doi = {10.1145/3573051.3596169},
abstract = {The arrival and development of digital technologies (e.g. the internet, personal computer) including mobile ones (e.g. smartphones) in recent years have raised both support for and concerns about the effects of adoption and usage on learning outcomes. The arguments come from different educational stakeholders including parents, educators, practitioners, policy makers, and learners themselves. On the one hand, those in favor argue for increasing opportunities to learn (OtL) or seamless learning thanks to the mobility and multifunctionality of mobile internet devices. On the other hand, those against attest to multitasking, task switching, and rising social media or entertainment addiction phenomena as detrimental. With evidence from Vietnam, this paper contributes to the scant literature of net effect of digital devices use, and identified insignificant effect of smartphone use on Math and reading comprehension test scores while positive results of internet and computer access, using linear regressions and Lewbel instrumental variable methods. The results can be explained by the significantly positive effects of internet use for exam samples searching while negative effect of internet use for social networking, which is one among many online platforms competing for attention in the attention economy era. Our results shed light on the intricacy of mobile internet use's effects for upper-secondary school students' key learning outcomes, and offer some recommendations for educational practitioners.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {267–271},
numpages = {5},
keywords = {ubiquitous technology, smartphones, secondary school, reading comprehension, math, internet, informal learning, educational achievements, digital use types, digital ecosystem, digital access, computers, cognitive skills, adolescents},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596170,
author = {Wu, Zhongdi and Larson, Eric and Sano, Makoto and Baker, Doris and Gage, Nathan and Kamata, Akihito},
title = {Towards Scalable Vocabulary Acquisition Assessment with BERT},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596170},
doi = {10.1145/3573051.3596170},
abstract = {In this investigation we propose new machine learning methods for automated scoring models that predict the vocabulary acquisition in science and social studies of second grade English language learners, based upon free-form spoken responses. We evaluate performance on an existing dataset and use transfer learning from a large pre-trained language model, reporting the influence of various objective function designs and the input-convex network design. In particular, we find that combining objective functions with varying properties, such as distance among scores, greatly improves the model reliability compared to human raters. Our models extend the current state of the art performance for assessing word definition tasks and sentence usage tasks in science and social studies, achieving excellent quadratic weighted kappa scores compared with human raters. However, human-human agreement still surpasses model-human agreement, leaving room for future improvement. Even so, our work highlights the scalability of automated vocabulary assessment of free-form spoken language tasks in early grades.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {272–276},
numpages = {5},
keywords = {transfer learning, natural language processing, human-machine reliability, deep neural networks, automated scoring},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596171,
author = {Manzak, Dilek and Joyner, David A.},
title = {Pre-Semester Predictors of Course Retention in a Large Online Graduate CS Program},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596171},
doi = {10.1145/3573051.3596171},
abstract = {Online education is convenient and accessible, but also brings new challenges to retention. Prior research has found that students in online programs---including CS programs---are more likely to withdraw both from individual classes as well as the program as a whole than students in traditional programs. In this study, we delve deeper into retention at the level of individual courses. We analyze three courses offered as part of a large online CS graduate degree program taught at a major research university in the United States. We obtained voluntary data from students across 12 semesters---including gender, prior CS experience, and anticipated workload for the course---and analyzed these variables in the context of course retention. While the average course drop rate for the examined courses in the program was 7\%, variations in this rate could be predicted by a number of different attributes: for instance, we observed that women, native English speakers, and students with less prior education were allmore likely to complete a course that they had started. Importantly, these predictors can be measured before the class begins, supporting early intervention.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {277–281},
numpages = {5},
keywords = {retention, online education, gender, English as a second language},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596172,
author = {Jordan, Katy and Damani, Kalifa and Myers, Christina and Zhao, Annette},
title = {The Use of SMS and Other Mobile Phone-based Messaging to Support Education at Scale: A Synthesis of Recent Evidence},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596172},
doi = {10.1145/3573051.3596172},
abstract = {Higher levels of device ownership and lower connectivity requirements are key reasons why mobile learning may have potential to support education at scale in low-income contexts. Interest in the use of mobile phones as an educational medium - particularly through the use of SMS or messaging apps - has been renewed recently, as a result of school closures prompted by the Covid-19 pandemic. As a result, the evidence base for educational interventions using SMS at scale has recently expanded. In this work-in-progress synthesis paper, we review recent research studies which have used SMS for education at scale, with findings published since the onset of the Covid-19 pandemic in 2020. We find that there has been a notable increase in studies which have used SMS to promote and support education at a large scale. In addition to its use as a medium for directly supporting learners, it has also been applied to promoting parental engagement and encouraging participation in formal schooling. The efficacy of interventions has been mixed, which highlights the need for nuance and further research as the field looks to understand which benefits of mobile learning could be beneficial to retain post-pandemic.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {282–286},
numpages = {5},
keywords = {mobile learning, low- and middle income countries, international development, educational technology, SMS},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596173,
author = {Inoue, Saki and Wang, Yuanyuan and Kawai, Yukiko and Sumiya, Kazutoshi},
title = {Encouraging Critical Thinking Support System: Question Generation and Lecture Slide Recommendations},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596173},
doi = {10.1145/3573051.3596173},
abstract = {In recent years, online learning has gained popularity in Japan, increasing opportunities for remote study using lecture slides. However, a common issue in Japanese education is that students tend to not ask questions in class. To address this issue, we propose an online learning support system informed by research on "critical thinking" that facilitates the process of students' questioning and thinking. In this study, we aimed to clarify the relationship between the questions generated by students and the class slides. We did this by analyzing the hierarchical relationship of the keywords that appear in the slides. We also propose a method of recommending appropriate lecture slides for the questions that we hope will encourage students to ask meaningful questions and engage in productive discussions.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {287–291},
numpages = {5},
keywords = {question generation, online learning support, lecture slide recommendation, e-learning, critical thinking},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596174,
author = {Nguyen, Ha and Diederich, Morgan},
title = {How Civil are Comments on TikTok's Educational Videos? Insights for Learning at Scale},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596174},
doi = {10.1145/3573051.3596174},
abstract = {Social networking sites present opportunities for informal learning at scale. Researchers have explored the use of TikTok, a site for sharing short-formed videos, for informal and formal learning. As we consider the learning potential of sites like TikTok, it is equally important to investigate features that may discourage learning. Our work examines the pervasiveness of incivility, or offensive language targeting individuals and ideas, in videos categorized with educational hashtags on TikTok. We collect a corpus of 346,626 TikTok comments, and find a small percentage (1.5\%) with uncivil content that might turn someone away from the discussion. To understand participation patterns, we compare comment threads in videos with the highest average incivility to the rest of the corpus. These comment threads generally have earlier appearance of uncivil content, lower participation from video creators, and less unique participation from different users. Findings have implications for content moderation in online discussions, to deprioritize uncivil content and encourage facilitation from content creators.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {292–296},
numpages = {5},
keywords = {social network, incivility, content moderation},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596175,
author = {Gomez, Manuel J. and Ruip\'{e}rez-Valiente, Jos\'{e} A. and Garc\'{\i}a Clemente, F\'{e}lix J.},
title = {Towards Game-based Assessment at Scale},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596175},
doi = {10.1145/3573051.3596175},
abstract = {Games are increasingly being recognized as valuable tools for learning. In addition, they are also being explored for their potential to provide valid and reliable assessments, as they allow to create authentic and engaging assessment contexts through interactive and immersive environments. However, there are challenges to enable Game-based Assessment (GBA) at scale, including the need for interoperability between assessment models and machinery, and the complexity of managing and processing large amounts of data generated by users' interaction with games. In this study, we propose a novel approach that combines the use of ontologies and Big Data technologies for developing interoperable GBAs. The architecture enables assessments to be performed using data from different games, and we also designed and implemented a service API that facilitates the Game-Based Assessment as a Service (GBAaaS) paradigm. GBAaaS simplifies the GBA development process and enables its adoption at scale, making it a promising approach for future developments in this field.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {297–301},
numpages = {5},
keywords = {interoperability, game-based assessment, educational technologies, data mining, big data},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596176,
author = {Zhang, Jiayi and Baker, Ryan S. and Farmer, Thomas},
title = {No Benefit for High-Dosage Time Management Interventions in Online Courses},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596176},
doi = {10.1145/3573051.3596176},
abstract = {In past work, time management interventions involving prompts, alerts, and planning tools have successfully nudged students in online courses, leading to higher engagement and improved performance. However, few studies have investigated the effectiveness of these interventions over time, understanding if the effectiveness maintains or changes based on dosage (i.e., how often an intervention is provided). In the current study, we conducted a randomized controlled trial to test if the effect of a time management intervention changes over repeated use. Students at an online computer science course were randomly assigned to receive interventions based on two schedules (i.e., high-dosage vs. low-dosage). We ran a two-way mixed ANOVA, comparing students' assignment start time and performance across several weeks. Unexpectedly, we did not find a significant main effect from the use of the intervention, nor was there an interaction effect between the use of the intervention and week of the course.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {302–305},
numpages = {4},
keywords = {time management, diminishing effect, behavioral interventions},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596177,
author = {Evrard, August E. and Matz, Rebecca and Murray, Erin and Hayward, Ben and Mills, Mark and Hayward, Caitlin},
title = {Innovating at Campus Scale: The Case of Michigan's Atlas},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596177},
doi = {10.1145/3573051.3596177},
abstract = {In 2016, a curricular information service was launched on the main campus of the University of Michigan. Through course guide entry links, students could view five-year historical summaries of various course properties. In subsequent years, the service expanded to share more elements of the academic landscape, including recent information on instructors and degrees. Rebranded as Atlas and positioned as an aid to improving academic decisions, this local campus innovation has scaled to be used by 99\% of undergraduate students, with more than 54,000 unique users in calendar year 2022. A schedule builder feature and the ability to create and share collections have driven user growth and offer unique research windows into student academic preferences and decisions. As Atlas pivots from sharing passive historical views to more actively guiding students' academic pathways, the faculty and staff team behind the service continue to experiment with community building, translational research, and maintaining innovation at scale. Here, we review the growth and evolution of the service and share the experience of innovating at scale at a major American research university.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {306–310},
numpages = {5},
keywords = {service design, data visualization, data governance, curricular analytics},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596178,
author = {K\"{o}hler, Daniel and Serth, Sebastian and Meinel, Christoph},
title = {On Air: Benefits of weekly Podcasts accompanying Online Courses},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596178},
doi = {10.1145/3573051.3596178},
abstract = {Podcasts are a widely-used medium for communication and learning. One advantage of them is the possibility to pursue other activities while listening. Contrasting, Massive Open Online Courses (MOOCs) employ video-based teaching methods. Current research, however, challenges the interactivity and variation of teaching content in established MOOCs. This manuscript presents an experiment conducted with a podcast series deployed alongside a MOOC on cybersecurity. In our Static-Group Comparison, we identified a significant increase in learning success in weekly graded exercises (6.3\%) and the course's final examination (6.4\%) for learners exposing themselves to the podcast. Our first study results are promising in favor of multimedia learning. Hence, we present ideas for additional analysis and briefly outline which aspects of the results should be discussed in more depth.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {311–315},
numpages = {5},
keywords = {secondary knowledge, podcasts, online education, mooc},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596179,
author = {Strukova, Sofia and Ruip\'{e}rez-Valiente, Jos\'{e} A. and G\'{o}mez M\'{a}rmol, F\'{e}lix},
title = {Towards the Identification of Experts in Informal Learning Portals at Scale},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596179},
doi = {10.1145/3573051.3596179},
abstract = {During the past decade, there has been growing interest among researchers in informal learning at scale, particularly in the area of expert finding. These platforms have played a fundamental role in facilitating informal learning at scale, by providing access to diverse expertise and knowledge resources that might not otherwise be available to learners. Based on the encountered gaps in expert identification in Question \&amp; Answer (Q&amp;A) portals, we inspect the feasibility of identifying data science experts in Reddit using the activity behaviour of every user, including Natural Language Processing (NLP), crowdsourced and user features sets. We also examine the impact of using only expert and non-expert classes versus three classes additionally including the out-of-scope class. Our findings can be used for distinguishing different types of users in Reddit, creating a recommendation system, identifying unreliable users or social bots in the early stage and reducing their influence.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {316–320},
numpages = {5},
keywords = {web mining, learning at scale, informal learning, computational social science, Reddit},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596180,
author = {Elhayany, Mohamed and Meinel, Christoph},
title = {Towards Automated Code Assessment with OpenJupyter in MOOCs},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596180},
doi = {10.1145/3573051.3596180},
abstract = {The popularity of Massive Open Online Courses (MOOCs) as a means of delivering education to large numbers of students has been growing steadily over the last decade. As technology improves, more educational content is becoming readily available to the public. JupyterLab, an open-source web-based interactive development environment (IDE), is also becoming increasingly popular in education, however, it is so far primarily used in small classroom settings. JupyterLab can provide a more interactive, hands-on, and collaborative learning experience for students in MOOCs, and it is highly customizable and can be accessed from anywhere. To capitalize on these benefits, we have developed OpenJupyter, which integrates JupyterLab at scale with MOOCs, enhancing the student learning experience and providing hands-on exercises for data science courses, making them more interactive and engaging. While MOOCs provide access to education for a large number of students, one of the significant challenges is providing effective and timely feedback to learners.&nbsp;OpenJupyter includes an auto-assessment capability that addresses this problem in MOOCs by automating the evaluation process and providing feedback to learners in a timely manner. In this paper, we provide an overview of the architecture of OpenJupyter, its scalability in the context of MOOCs, and its effectiveness in addressing the auto-assessment challenge. We also discuss the Advantages and limitations associated with using OpenJupyter in a MOOC context and provide a reference for educators and researchers who wish to implement similar tools. Our efforts aim to foster an open educational environment in the field of programming by providing learners with an interactive learning tool and a streamlined technical setup, allowing them to acquire and test their knowledge at their own pace.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {321–325},
numpages = {5},
keywords = {programming, openjupyter, auto-assessment, MOOC, JupyterLab},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596181,
author = {Liu, Hahohua and Feng, Shihui and Qiao, Chen},
title = {A Machine Learning Approach for Understanding the Educational Foci and Technical Solutions of AIED},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596181},
doi = {10.1145/3573051.3596181},
abstract = {This work-in-progress paper employs a machine learning method for the automated analysis of research interests in Artificial Intelligence in Education (AIED) at scale. We aim to analyze the essential techniques and critical educational problems studied by researchers in five AIED-related conferences and journals between 2010 and 2022. We trained and compared different machine learning models and feature extraction techniques to achieve the research objective. After comparing different models and hyperparameter combinations, our classifier achieves an accuracy of 0.87 and Cohen's kappa of 0.80. Based on the classification results, we identified the top 10 most frequent keywords within each category for every four year period over the past 12 years. Using the classifier, the 10,723 keywords from 2,684 articles were classified into three categories: educational foci, technical solutions, and AIED applications. We find that 'natural language processing' and 'machine learning' are the primary technical keywords in AIED research, and 'deep learning' and 'artificial intelligence' are the trending technical keywords since 2017. Meanwhile, 'massive open online courses', 'self-regulated learning', 'feedback', 'collaborative learning', and 'online learning' are the top educational foci in the field over the last 12 years. 'Intelligent tutoring systems', 'educational data mining', 'knowledge tracing', and 'learning analytics' continue to receive attention as AIED applications of sustained interest. This study helps to understand the educational foci and technical solutions of AIED research at scale and provides insights into the future of AIED research.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {326–330},
numpages = {5},
keywords = {machine learning, classification, artificial intelligence in education},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596182,
author = {Wang, Guan-Yun and Nagata, Hikaru and Hatori, Yasuhiro and Sato, Yoshiyuki and Tseng, Chia-Huei and Shioiri, Satoshi},
title = {Detecting Learners' Hint Inquiry Behaviors in e-Learning Environment by using Facial Expressions},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596182},
doi = {10.1145/3573051.3596182},
abstract = {It is difficult for teachers to track how each student is engaged in a class through e-learning. The current study simulated an e-learning environment by designing a website which contains interactable functions and hint buttons to examine whether the mental state of hint seeking can be estimated from facial features. We currently recruited 9 participants and asked them to solve a problem used in the International Olympiad of Linguistics in 2018. In this study, participants learned alone without discussing with others on a website designed for the experiment. While they were solving the problems, their face images were recorded. We extracted their facial features as Action Unit codes (AUs) by an open-source software, OpenFace. There was a function that provided hints to participants when he or she clicked a button. We attempted to estimate the time to click the button, that is the time when a hint was required, from facial expressions using a machine learning technique. We used LightGBM for the prediction. The model trained by LightGBM classified the state of "seeking hints" and the other states with an accuracy of 85.0\%. The SHAP (SHapley Additive exPlanations) values, which are indexes of contribution levels, showed that facial features of "brow lowerer", "lip tightener", and "jaw drop" are the top three important features for the classification. This suggests that it is possible to build a system that detects the mental state of hint-seeking from facial features.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {331–335},
numpages = {5},
keywords = {machine learning, hint processing, facial expression, action units codes},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596183,
author = {Kim, Dae-Eun and Hong, Changki and Kim, Woo Hyun},
title = {Efficient Transformer-based Knowledge Tracing for a Personalized Language Education Application},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596183},
doi = {10.1145/3573051.3596183},
abstract = {The purpose of this paper is to propose a new deep learning-based approach to recommend highly personalized educational contents to learners. Towards this goal, we present a knowledge tracing algorithm by adding long short-term memory units to a Transformer-based model. By inferring the knowledge state of a learner through the proposed KT algorithm, it not only removes problems that the learner does not have to solve but also suggest problems so that the learner's knowledge state level improves most efficiently. In this manner, a personalized educational curriculum can be provided to each learner. We trained the model with 90 million datasets collected from a Hangeul (i.e., Korean character) learning mobile application called "Sojung-Hangeul", one of the market-leading Korean learning services. The experimental results show that the AUC of the proposed model significantly improves from 0.88 to 0.92 compared to the recent Transformer-based approach in real-time environments. The proposed deep learning model is applied into "Sojung-Hangeul", and the application is currently available at https://bit.ly/Sojung-Hangeul.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {336–340},
numpages = {5},
keywords = {transformer, recommendation system, personalized education, knowledge tracing},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596184,
author = {Nguyen, David Van and Epstein, Daniel A. and Doroudi, Shayan},
title = {Effects of Scaling Up Apprentice-Style Research: Perceptions from Mentors and Mentees},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596184},
doi = {10.1145/3573051.3596184},
abstract = {Access to undergraduate research is limited. One approach to broaden access is scaling up the mentee-to-mentor ratio (e.g., course-based undergraduate research experiences have a classroom of student mentees being led by one professor mentor). However, some mentors and mentees may prefer apprentice-style research, which is defined as research with a small mentee-to-mentor ratio. Pulling influences from non-scaled and scaled approaches, we implemented and evaluated the Community College to PhD (CC2PhD) Scholars Program, which was a community college research program. CC2PhD was designed to scale up non-personalized aspects of apprentice-style research while maintaining personalized one-on-one mentoring. The scaled non-personalized aspects of CC2PhD included the predefined mentoring curriculum and the research methods workshops. They were "scaled" in the sense that few people were involved in curriculum development and workshop instruction. These scaled resources can then be used by a large number of mentor-mentee pairs. We interviewed and surveyed seven mentor alumni and six mentee alumni to understand the effects of the scaled aspects of CC2PhD. We identified four themes: (1) improved time-related issues by saving time and facilitating time management, (2) influenced meeting content, (3) helped beginner mentors and mentees, and (4) increased mentors' willingness to volunteer. Future researchers can further scale-up and digitize our scaled research-apprentice model. For example, the mentoring curriculum and workshops can be adapted into a MOOC, which mentor-mentee pairs can reference from.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {341–345},
numpages = {5},
keywords = {undergraduate research, mentoring, community college to PhD scholars program, apprentice-style research at scale},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596185,
author = {Hadi Mogavi, Reza and Hoffman, Jennifer and Deng, Chao and Du, Yiwei and Haq, Ehsan-Ul and Hui, Pan},
title = {Envisioning an Inclusive Metaverse: Student Perspectives on Accessible and Empowering Metaverse-Enabled Learning},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596185},
doi = {10.1145/3573051.3596185},
abstract = {The emergence of the metaverse is being widely viewed as a revolutionary technology owing to a myriad of factors, particularly the potential to increase the accessibility of learning for students with disabilities. However, not much is yet known about the views and expectations of disabled students in this regard. The fact that the metaverse is still in its nascent stage exemplifies the need for such timely discourse. To bridge this important gap, we conducted a series of semi-structured interviews with 56 university students with disabilities in the United States and Hong Kong to understand their views and expectations concerning the future of metaverse-driven education. We have distilled student expectations into five thematic categories, referred to as the REEPS framework: Recognition, Empowerment, Engagement, Privacy, and Safety. Additionally, we have summarized the main design considerations in eight concise points. This paper is aimed at helping technology developers and policymakers plan ahead of time and improving the experiences of students with disabilities.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {346–353},
numpages = {8},
keywords = {metaverse, interview study, inclusion, higher education, emerging technologies, disabled students, accessibility},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596186,
author = {Ouhaichi, Hamza and Spikol, Daniel and Vogel, Bahtijar},
title = {Rethinking MMLA: Design Considerations for Multimodal Learning Analytics Systems},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596186},
doi = {10.1145/3573051.3596186},
abstract = {Designing MMLA systems is a complex task requiring a wide range of considerations. In this paper, we identify key considerations that are essential for designing MMLA systems. These considerations include data management, human factors, sensors and modalities, learning scenarios, privacy and ethics, interpretation and feedback, and data collection. The implications of these considerations are twofold: 1) The need for flexibility in MMLA systems to adapt to different learning contexts and scales, and 2) The need for a researcher-centered approach to designing MMLA systems. Unfortunately, the sheer number of considerations can lead to a state of "analysis paralysis," where deciding where to begin and how to proceed becomes overwhelming. This synthesis paper asks researchers to rethink the design of MMLA systems and aims to provide guidance for developers and practitioners in the field of MMLA.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {354–359},
numpages = {6},
keywords = {system design, scalability, multimodal learning analytics, internet of things},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596187,
author = {Basu Mallick, Debshila and Bradford, Brittany C. and Baraniuk, Richard},
title = {Secure Education and Learning Research at Scale with OpenStax Kinetic},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596187},
doi = {10.1145/3573051.3596187},
abstract = {OpenStax Kinetic is an innovative research infrastructure that aims to transform education and learning research in the digital age. With its access to large sample sizes, authentic learning environments, experimental control, scalability, security and privacy protection, Kinetic provides an unparalleled opportunity for researchers to study the complex interactions between different factors in digital learning environments. This versatile platform utilizes Qualtrics and can support various research designs, including correlational, longitudinal, and interventional studies. Kinetic's unique privacy-by-design implementation via secure enclaves ensures that researchers can analyze fully-identified data without compromising data security and privacy as well as affords greater analytical reproducibility. The findings from Kinetic can inform educational interventions and strategies to enhance student success in digital learning environments. Kinetic has the potential to significantly advance education and learning research by improving pedagogies, practices, and policies in education and learning sciences. In this demo of Kinetic, researchers will be able to interact with the test instance of the Kinetic system online and view the learner experience. All researchers will be able to engage in the experience of creating a study, releasing a study, and interacting with our implementation of secure enclaves for data analysis.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {360–362},
numpages = {3},
keywords = {research methods at scale, education R&amp;D, digital learning research, digital learning platform, adult learning},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596188,
author = {Bradford, Brittany C. and Basu Mallick, Debshila and Baraniuk, Richard G.},
title = {Unlocking Financial Success: Empowering Higher Ed Students and Developing Financial Literacy Interventions at Scale},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596188},
doi = {10.1145/3573051.3596188},
abstract = {Greater financial literacy is critically needed among young adults in the United States [10,35], but many financial literacy education courses have been less effective than hoped for by educators and researchers [7,15]. Additionally, many have not been designed around established curricula or learning science principles, rendering findings difficult for researchers to study empirically [6,34]. In order to better understand the psychosocial mechanisms that predict success in improving learner knowledge and behavior, online educational interventions at scale can be an effective path forward. We conducted interviews with subject matter experts and young adult students to explore the highest priority learning objectives for a brief course curriculum to improve the financial literacy of US young adults. We then leveraged our findings from this study and content from our open-source textbooks to develop the first of several brief online learning interventions for deployment on the large-scale OpenStax Kinetic research infrastructure [2]. In this work-in-progress paper, we discuss the next steps in our research agenda, including course content development and deploying this intervention, as well as our broader plans for our future financial literacy education interventions and translating research into practice with our institutional collaborations.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {363–367},
numpages = {5},
keywords = {research methods at scale, qualitative data analysis, interventions, higher education, financial literacy, adult learning},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596189,
author = {Bradford, Brittany C.},
title = {Does STEM Success Start Young? Exploring Higher Ed Students' Early Academic Experiences in Science and Math at Scale},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596189},
doi = {10.1145/3573051.3596189},
abstract = {The United States is experiencing a shortage of STEM workers [24], with many students leaving the pipeline before attaining a career in STEM [18]. STEM education researchers have identified factors at the high school and college level that contribute to attrition, but earlier life events remain underexplored [14]. In this work-in-progress paper, we examine childhood experiences through the lens of qualitative analyses and discuss our ongoing development of an overall understanding of the relevant life and academic "themes" that shape students' lives before entering secondary school. We are currently testing our seven-theme model on the OpenStax Kinetic large-scale research infrastructure using quantitative surveys of participants' biographical data. Our findings from this study will inform future refinement of the survey and its themes, with a particular emphasis on understanding the influence of contextual demographic and psychosocial factors. Over the longer-term, we hope to support research that identifies critical early STEM experiences and offers insight into where certain students might benefit the most from additional STEM support or experiences.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {368–372},
numpages = {5},
keywords = {research methods at scale, qualitative data, elementary school student experiences, biographical data, STEM},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596190,
author = {Hornback, Andrew and Buckley, Stephen and Kos, John and Bunin, Scott and An, Sungeun and Joyner, David and Goel, Ashok},
title = {A Scalable Architecture for Conducting A/B Experiments in Educational Settings},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596190},
doi = {10.1145/3573051.3596190},
abstract = {A/B experiments are commonly used in research to compare the effects of changing one or more variables in two different experimental groups-a control group and a treatment group. While the benefits of using A/B experiments are widely known and accepted in education, there is less agreement on an approach to creating software infrastructure systems to assist in rapidly conducting such experiments in the field. To assist in alleviating this gap, we are creating a software infrastructure for A/B experiments that allows researchers to conduct experiments and automatically analyze their results for an education-focused ecology-based conceptual modeling platform.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {373–377},
numpages = {5},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596191,
author = {Smolansky, Adele and Cram, Andrew and Raduescu, Corina and Zeivots, Sandris and Huber, Elaine and Kizilcec, Rene F.},
title = {Educator and Student Perspectives on the Impact of Generative AI on Assessments in Higher Education},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596191},
doi = {10.1145/3573051.3596191},
abstract = {The sudden popularity and availability of generative AI tools, such as ChatGPT that can write compelling essays on any topic, code in various programming languages, and ace standardized tests across domains, raises questions about the sustainability of traditional assessment practices. To seize this opportunity for innovation in assessment practice, we conducted a survey to understand both the educators' and students' perspectives on the issue. We measure and compare attitudes of both stakeholders across various assessment scenarios, building on an established framework for examining the quality of online assessments along six dimensions. Responses from 389 students and 36 educators across two universities indicate moderate usage of generative AI, consensus for which types of assessments are most impacted, and concerns about academic integrity. Educators prefer adapted assessments that assume AI will be used and encourage critical thinking, but students' reaction is mixed, in part due to concerns about a loss of creativity. The findings show the importance of engaging educators and students in assessment reform efforts to focus on the process of learning over its outputs, higher-order thinking, and authentic applications.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {378–382},
numpages = {5},
keywords = {survey, students, generative AI, educators, assessment, ChatGPT},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596192,
author = {Duncan, Alex and Rusch, Ana and Ravi, Prerna and Joyner, David},
title = {The L@St Eight Years: A Review of Papers and Authors at Learning @ Scale},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596192},
doi = {10.1145/3573051.3596192},
abstract = {We examine trends in the Learning at Scale conference from 2014 through 2021. We use an original coding scheme to classify all 142 full papers from five angles: setting, approach, pedagogical strategy, population of interest, and dependent variable. We observe a decline of research on MOOCs, an increase in number of settings studied over time, and a consistent focus on assessment strategies. We then examine other conferences to which Learning at Scale authors contribute research. This paper contributes an original coding scheme to classify future Learning at Scale papers; an analysis of the research focuses at Learning at Scale over time; and an evaluation of the mutual influence between Learning at Scale and other venues. These latter two contributions contextualize learning at scale research within Learning at Scale and the broader research community to show how the field has evolved and to help predict its future directions.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {383–387},
numpages = {5},
keywords = {synthesis, pedagogical strategy, future of learning at scale},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596193,
author = {Duncan, Alex and Joyner, David},
title = {Ready or Not, Here I Computer Science: Trends in Preparatory Work Pursued by Incoming Students in an Online Graduate Computer Science Program},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596193},
doi = {10.1145/3573051.3596193},
abstract = {Research on how students prepare for graduate computer science programs typically focuses on single, subject-specific interventions or relates to preparation for life as a graduate student. Preparatory work completed prior to enrolling in such a program can be particularly important for underrepresented minorities and those without technical backgrounds. We use survey data from incoming students in a large online graduate computer science program to answer three research questions: What are the backgrounds of students entering the program? How do students prepare for the program? And how does student preparation differ based on demographics and prior experience? We find that: male students are more likely than female students to enter the program with computer science qualifications; older students, female students, and those with non-technical degrees are more likely to pursue preparation; and students with no online learning experience are less likely to pursue preparation. These findings highlight the importance of student backgrounds when creating preparatory courses and indicate the value of preparatory courses in increasing diversity in large online graduate programs.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {388–392},
numpages = {5},
keywords = {underrepresented minorities, prep courses, online learning, diversity, MOOCs},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596194,
author = {Cram, Andrew and Raduescu, Corina and Zeivots, Sandris and Smolansky, Adele and Kizilcec, Ren\'{e} F. and Huber, Elaine},
title = {Developing a Prototype to Scale up Digital Support for Online Assessment Design},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596194},
doi = {10.1145/3573051.3596194},
abstract = {Educators rarely have access to just-in-time feedback and guiding heuristics when designing or updating assessments in higher education. This study describes the initial development process for an automated support system for designing high-quality online assessments. We identify key elements to embed in this digital artifact to offer just-in-time support for educators to design and evaluate their online assessments. We follow a design science approach in six stages, because it simultaneously generates knowledge about the method used to develop the artifact and the design of the artifact itself. Specifically, we focus on the early stages of problem identification, solution objectives, and initial conceptual design. After reviewing multiple assessment models and frameworks, we discuss a recent framework for evaluating and designing high-quality online assessments, consisting of ten design and contextual elements. This framework underpins the proposed solution which is a digital artifact that encourages consideration of alternate forms of assessment while retaining the flexibility to operate within individual educators' design practices and contexts. We expect the proposed system to help educators and instructional designers to better understand the strengths and weaknesses of their assessments, consider alternate forms of assessment, and incorporate the system into their assessment design process.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {393–397},
numpages = {5},
keywords = {online assessment design, instructional design, educational development, digital artifact, design science research},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596195,
author = {Kok, Christopher and Wang, Xu},
title = {Comb: Giving Feedback to Short Answer at Scale with Human-in-the-Loop Rubric Creation},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596195},
doi = {10.1145/3573051.3596195},
abstract = {As enrollment in college classes rises, it is increasingly difficult to grade and provide feedback to open-ended assignments. It is a timeconsuming and labor-intensive task for instructors, especially when the grading criteria are subjective and constantly evolving. Rubrics are often used to standardize grading, but they can be challenging to create and may not always capture the nuances of a particular assignment. Additionally, it can be difficult to articulate principles or constraints that define a "good" solution in less well-defined domains; like human-computer interaction (HCI) [3, 8] or user experience (UX) [7]. Instructors may delegate the task of grading and offering feedback to a number of graders. However, through our co-design study (section 2.1), we've found that inter-grader reliability, managing time constraints, and dealing with unclear rubrics are just some of the many issues faced by graders in this process.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {398–400},
numpages = {3},
keywords = {short answer grading, human-in-the-loop, educational technology, automatic feedback, applied natural language processing},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596196,
author = {Hung, Jui-Tse and Cui, Christopher and Agarwal, Varun and Chatterjee, Saurabh and Apoorv, Raghav and Graziano, Rocko and Starner, Thad},
title = {Examinator v3.0: Cheating Detection in Online Take-Home Exams},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596196},
doi = {10.1145/3573051.3596196},
abstract = {Examinator v3.0 detects cheating in online take-home exams by comparing answers and the timestamps they were entered. A web interface enables efficient manual inspection. Use of the tool reveals that certain question types substantially enhance cheating detection, demonstrating the potential of automated algorithmic detection at scale. Examinator v3.0 has analyzed 915,831 pairs of exam submissions across three courses over two semesters at a top U.S. institution, identifying 46 instances of cheating.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {401–405},
numpages = {5},
keywords = {take-home exams, online learning, cheating detection, academic integrity},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596197,
author = {Irish, India and Chatterjee, Saurabh and Jivani, Sheliza and Jia, Xiangyu and Lee, Jeonghyun and Arriaga, Rosa and Starner, Thad},
title = {Managing the Chaos: Approaches to Navigating Discussion Forums for Instructional Staff},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596197},
doi = {10.1145/3573051.3596197},
abstract = {As enrollment increases, teaching assistants (TAs) need help prioritizing responding to students' posts in on-line forums. In a graduate-level Artificial Intelligence forum, three instructors rank the urgency of posts which is compared to the ratings of 13 course TAs; correlation with the instructors' scores have an r=0.55, with a TA inter-rater reliability of 35\%. However, when TAs used a codebook containing seven dimensions created by the instructors to define urgency levels, correlation increased to r=0.73 and reliability to 53\%. The instructor rankings are also compared to cognitive presence ratings from the Community of Inquiry framework. Cognitive presence correlates with urgency with r=-0.68. These results suggest that recommendation agents that prioritize posts based on cognitive presence or the urgency codebook may be beneficial for TAs.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {406–410},
numpages = {5},
keywords = {e-learning, distant learning},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596198,
author = {Henkel, Owen and Hills, Libby},
title = {Leveraging Human Feedback to Scale Educational Datasets: Combining Crowdworkers and Comparative Judgement},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596198},
doi = {10.1145/3573051.3596198},
abstract = {Machine Learning models have many potentially beneficial applications in education settings, but a key barrier to their development is securing enough high-quality, labelled data to train these models. This process has traditionally relied on highly skilled raters using complex, multi-class rubrics, which made labelling expensive and difficult to scale. A more scalable approach would be to use non-expert crowdworkers to evaluate student work, but maintaining high levels of accuracy and inter-rater reliability when using non-expert workers can be challenging. This paper reports on two experiments in which non-expert crowdworkers hired to evaluate (i.e., score) student work and were randomly assigned to one of two conditions: the control, where they were asked to assign a rubrics based score (i.e., a categorical judgement), or the treatment, where they were shown the same student answers, but were asked to decide which of two candidate answers was better (i.e., a comparative/preference-based judgement). We found that using comparative judgement substantially improved inter-rater reliability on both tasks. These results are in-line with well-established literature on the benefits of comparative judgement in the field of educational assessment, as well as with recent trends in artificial intelligence research, where comparative judgement is becoming the preferred method for providing human feedback on model outputs. These results are novel and important in demonstrating the effects of using the combination of comparative judgement and crowdworkers to evaluate educational data},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {411–415},
numpages = {5},
keywords = {human feedback, crowdworkers, comparative judgement},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596200,
author = {Han, Jieun and Yoo, Haneul and Kim, Yoonsu and Myung, Junho and Kim, Minsun and Lim, Hyunseung and Kim, Juho and Lee, Tak Yeon and Hong, Hwajung and Ahn, So-Yeon and Oh, Alice},
title = {RECIPE: How to Integrate ChatGPT into EFL Writing Education},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596200},
doi = {10.1145/3573051.3596200},
abstract = {The integration of generative AI in the field of education is actively being explored. In particular, ChatGPT has garnered significant interest, offering an opportunity to examine its effectiveness in English as a foreign language (EFL) education. To address this need, we present a novel learning platform called RECIPE (Revising an Essay with ChatGPT on an Interactive Platform for EFL learners). Our platform features two types of prompts that facilitate conversations between ChatGPT and students: (1) a hidden prompt for ChatGPT to take an EFL teacher role and (2) an open prompt for students to initiate a dialogue with a self-written summary of what they have learned. We deployed this platform for 213 undergraduate and graduate students enrolled in EFL writing courses and seven instructors. For this study, we collect students' interaction data from RECIPE, including students' perceptions and usage of the platform, and user scenarios are examined with the data. We also conduct a focus group interview with six students and an individual interview with one EFL instructor to explore design opportunities for leveraging generative AI models in the field of EFL education.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {416–420},
numpages = {5},
keywords = {learner-ChatGPT interaction, generative AI, essay writing, EFL learners, ChatGPT},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596201,
author = {Aggarwal, Ashish and Pitts, Griffin and Marusic, Shayne and Harvey, Leslie and Gardner-McCune, Christina},
title = {Does the Availability of Reattempts and Video Solutions Affect Learners' Voluntary Engagement with Mastery Learning Activities?},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596201},
doi = {10.1145/3573051.3596201},
abstract = {Designing interactive virtual learning environments with effective and engaging design elements is crucial in enhancing learners' motivation, engagement, and learning outcomes. By prioritizing a positive user experience that curates cognitive load, interactions within virtual learning environments may effectively promote voluntary and formative engagement. This analysis focuses on investigating the affordances of having an opportunity to immediately reattempt an incorrectly answered question and additionally have access to video solutions on students' voluntary engagement with mastery learning activities. These mastery learning activities were provided in the form of quizzes through a virtual learning environment, YANTRA EDU. This application was developed to facilitate mastery learning, where learners have the opportunity to engage with the sequential practice of various concepts in an introductory programming (CS1) course.The paper describes the design of YANTRA EDU and presents a comparative study of two design conditions, examining the variations in student interactions between the two conditions. The study is based on the availability of reattempts and video-based scaffolding as key differentiating factors in the two conditions. In the first condition (NoRV), the questions did not have an immediate reattempt option if an answer was incorrect. It also did not have any video-based solution available to the students. In the second condition (YesRV), students were provided with one reattempt opportunity if an answer was incorrect. And if the second reattempt response was incorrect, they were provided a video that included a step-by-step explanation of the question's correct approach and solution. In both conditions, the questions would randomly repeat after all the unique questions were attempted to provide continuous practice. We analyze and compare students' aggregate voluntary usage of the practice quizzes between the two conditions. We find that although there were no statistically significant differences in the number of attempted questions and the number of questions correctly answered in the two conditions, students exhibited statistically significant higher accuracy rates with the NoRV condition, where they did not have the option to reattempt or receive video solutions. These results contribute to the understanding of how different interaction design components may influence learners' voluntary engagement within virtual learning environments.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {421–425},
numpages = {5},
keywords = {voluntary engagement, virtual learning environment, video-based-scaffolding, reattempts, optional practice, mastery learning, interaction, formative, design, CS1},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3573051.3596202,
author = {MacDonagh, Aidan},
title = {Increasing Available Attempts: Changes in Student Correctness on Formative Introductory Physics Problems},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596202},
doi = {10.1145/3573051.3596202},
abstract = {Student submission data from formative graded physics problems were analyzed to detect effects from increasing the number of available attempts. The problems were used in two subsequent runs of MIT's general requirement introductory electricity and magnetism course, 8.02, with 190 students in the 2021 course and 203 in the 2022 course. Students completed the problems asynchronously in interactive online lessons as part of a blended learning design. The problems awarded full credit on any attempt and gave correctness as submission feedback. A number of available attempts was set for each problem by the course designers, varying across problems. Between 2021 and 2022, this number was increased for some problems but not others, creating a natural experiment. Qualitative effects were evaluated relative to a control group of unmodified problems and were found to differ between constructed response and selected response problems. Constructed response problems saw a significant decrease in first-attempt success rate and increase in rate of abandoning with attempts remaining, despite an insignificant increase in overall success rate. Selected response problems saw a significant increase in overall success but negligible changes in first-attempt success and abandoning. For both types, there was a significant increase in the overall number of attempts used. These results suggest that increasing attempts for constructed and selected response problems may encourage undesirable approaches to answering the problems, even in a formative context.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {426–430},
numpages = {5},
keywords = {online problems, multiple attempts, introductory physics, formative assessment, constructed and selected response, blended learning},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

